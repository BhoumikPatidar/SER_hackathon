{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport random\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\nrandom.seed(42)\n\n# Configuration\nTARGET_SR = 16000\nTARGET_DURATION = 3.0  # seconds\nSAMPLES_PER_EMOTION = 30\nOUTPUT_DIR = 'validation_dataset'\nAUDIO_DIR = os.path.join(OUTPUT_DIR, 'audio_files')\n\n# Create output directories\nos.makedirs(AUDIO_DIR, exist_ok=True)\n\nprint(\"🎵 Speech Emotion Recognition Validation Dataset Generator\")\nprint(\"=\" * 60)\n\n# ================================\n# 1. DATASET LOADING AND EXPLORATION\n# ================================\n\ndef load_crema_d(base_path):\n    \"\"\"Load CREMA-D dataset information\"\"\"\n    audio_files = []\n    crema_path = base_path\n    \n    # CREMA-D emotion mapping\n    emotion_map = {\n        'ANG': 'angry',\n        'DIS': 'disgust', \n        'FEA': 'fear',\n        'HAP': 'happy',\n        'NEU': 'neutral',\n        'SAD': 'sad'\n    }\n    \n    for file in os.listdir(crema_path):\n        if file.endswith('.wav'):\n            parts = file.split('_')\n            if len(parts) >= 3:\n                actor_id = parts[0]\n                emotion_code = parts[2]\n                \n                if emotion_code in emotion_map:\n                    audio_files.append({\n                        'filename': file,\n                        'filepath': os.path.join(crema_path, file),\n                        'emotion': emotion_map[emotion_code],\n                        'actor_id': actor_id,\n                        'dataset': 'CREMA-D',\n                        'original_emotion_code': emotion_code\n                    })\n    \n    return audio_files\n\ndef load_ravdess(base_path):\n    \"\"\"Load RAVDESS dataset information\"\"\"\n    audio_files = []\n    \n    # RAVDESS emotion mapping (from filename position 3)\n    emotion_map = {\n        '01': 'neutral',\n        '02': 'calm', \n        '03': 'happy',\n        '04': 'sad',\n        '05': 'angry',\n        '06': 'fear',\n        '07': 'disgust',\n        '08': 'surprise'\n    }\n    \n    # Find all audio files in RAVDESS structure\n    for root, dirs, files in os.walk(base_path):\n        for file in files:\n            if file.endswith('.wav') and file.startswith('03-01-'):  # Audio-only speech files\n                parts = file.split('-')\n                if len(parts) >= 7:\n                    emotion_code = parts[2]\n                    actor_id = parts[6].split('.')[0]\n                    \n                    if emotion_code in emotion_map:\n                        # Map 'calm' to 'neutral' for consistency\n                        emotion = emotion_map[emotion_code]\n                        if emotion == 'calm':\n                            emotion = 'neutral'\n                            \n                        audio_files.append({\n                            'filename': file,\n                            'filepath': os.path.join(root, file),\n                            'emotion': emotion,\n                            'actor_id': f\"R{actor_id}\",  # Prefix to distinguish from CREMA-D\n                            'dataset': 'RAVDESS',\n                            'original_emotion_code': emotion_code\n                        })\n    \n    return audio_files\n\n# Load datasets\nprint(\"📂 Loading CREMA-D dataset...\")\ncrema_files = load_crema_d('/kaggle/input/cremad/AudioWAV')\nprint(f\"   Found {len(crema_files)} CREMA-D files\")\n\nprint(\"📂 Loading RAVDESS dataset...\")\nravdess_files = load_ravdess('/kaggle/input/ravdess-emotional-speech-audio')\nprint(f\"   Found {len(ravdess_files)} RAVDESS files\")\n\n# Combine and analyze\nall_files = crema_files + ravdess_files\ndf_all = pd.DataFrame(all_files)\n\nprint(\"\\n📊 Dataset Overview:\")\noverview_table = df_all.groupby(['emotion', 'dataset']).size().unstack(fill_value=0)\nprint(overview_table)\nprint(f\"\\nTotal files loaded: {len(df_all)}\")\nprint(f\"Emotions available in both datasets: {set(df_all[df_all['dataset']=='CREMA-D']['emotion']) & set(df_all[df_all['dataset']=='RAVDESS']['emotion'])}\")\nprint(f\"Emotions only in RAVDESS: {set(df_all[df_all['dataset']=='RAVDESS']['emotion']) - set(df_all[df_all['dataset']=='CREMA-D']['emotion'])}\")\nprint(f\"Emotions only in CREMA-D: {set(df_all[df_all['dataset']=='CREMA-D']['emotion']) - set(df_all[df_all['dataset']=='RAVDESS']['emotion'])}\")\n\n# ================================\n# 2. BALANCED SAMPLING STRATEGY\n# ================================\n\ndef sample_balanced_dataset(df, samples_per_emotion=30):\n    \"\"\"Sample a balanced dataset with speaker variety from both datasets\"\"\"\n    final_samples = []\n    \n    # Define target emotions\n    target_emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n    \n    for emotion in target_emotions:\n        print(f\"\\n🎯 Sampling {emotion} emotion:\")\n        \n        if emotion == 'surprise':\n            # Surprise only from RAVDESS (CREMA-D doesn't have surprise)\n            emotion_files = df[(df['emotion'] == emotion) & (df['dataset'] == 'RAVDESS')].copy()\n            print(f\"   Available RAVDESS files: {len(emotion_files)}\")\n            \n            if len(emotion_files) == 0:\n                print(f\"   ⚠️  No files found for {emotion}\")\n                continue\n                \n            # Sample from RAVDESS only\n            if len(emotion_files) >= samples_per_emotion:\n                sampled = emotion_files.sample(n=samples_per_emotion, random_state=42).to_dict('records')\n            else:\n                sampled = emotion_files.to_dict('records')\n                print(f\"   ⚠️  Only {len(sampled)} files available (less than {samples_per_emotion})\")\n        \n        else:\n            # For other emotions, sample from BOTH datasets\n            crema_files = df[(df['emotion'] == emotion) & (df['dataset'] == 'CREMA-D')].copy()\n            ravdess_files = df[(df['emotion'] == emotion) & (df['dataset'] == 'RAVDESS')].copy()\n            \n            print(f\"   Available CREMA-D files: {len(crema_files)}\")\n            print(f\"   Available RAVDESS files: {len(ravdess_files)}\")\n            \n            # Calculate how many samples from each dataset\n            total_available = len(crema_files) + len(ravdess_files)\n            if total_available == 0:\n                print(f\"   ⚠️  No files found for {emotion}\")\n                continue\n            \n            # Aim for roughly equal split between datasets when possible\n            if len(crema_files) > 0 and len(ravdess_files) > 0:\n                # Both datasets have this emotion\n                crema_samples = min(samples_per_emotion // 2, len(crema_files))\n                ravdess_samples = min(samples_per_emotion - crema_samples, len(ravdess_files))\n                \n                # If one dataset can't provide enough, take more from the other\n                if crema_samples < samples_per_emotion // 2 and len(ravdess_files) > ravdess_samples:\n                    additional_needed = (samples_per_emotion // 2) - crema_samples\n                    ravdess_samples = min(ravdess_samples + additional_needed, len(ravdess_files))\n                elif ravdess_samples < (samples_per_emotion - samples_per_emotion // 2) and len(crema_files) > crema_samples:\n                    additional_needed = samples_per_emotion - crema_samples - ravdess_samples\n                    crema_samples = min(crema_samples + additional_needed, len(crema_files))\n                \n                print(f\"   Sampling {crema_samples} from CREMA-D, {ravdess_samples} from RAVDESS\")\n                \n                sampled = []\n                if crema_samples > 0:\n                    sampled.extend(crema_files.sample(n=crema_samples, random_state=42).to_dict('records'))\n                if ravdess_samples > 0:\n                    sampled.extend(ravdess_files.sample(n=ravdess_samples, random_state=42).to_dict('records'))\n                    \n            elif len(crema_files) > 0:\n                # Only CREMA-D has this emotion\n                n_samples = min(samples_per_emotion, len(crema_files))\n                sampled = crema_files.sample(n=n_samples, random_state=42).to_dict('records')\n                print(f\"   Sampling {n_samples} from CREMA-D only\")\n                \n            elif len(ravdess_files) > 0:\n                # Only RAVDESS has this emotion\n                n_samples = min(samples_per_emotion, len(ravdess_files))\n                sampled = ravdess_files.sample(n=n_samples, random_state=42).to_dict('records')\n                print(f\"   Sampling {n_samples} from RAVDESS only\")\n                \n            else:\n                sampled = []\n        \n        print(f\"   ✅ Selected: {len(sampled)} files\")\n        if len(sampled) > 0:\n            datasets_used = set([s['dataset'] for s in sampled])\n            actors_used = len(set([s['actor_id'] for s in sampled]))\n            print(f\"   📊 Datasets used: {', '.join(datasets_used)}\")\n            print(f\"   🎭 Unique actors: {actors_used}\")\n        \n        final_samples.extend(sampled)\n    \n    return final_samples\n\n# Sample balanced dataset\nprint(\"\\n🎲 Creating balanced sample...\")\nselected_samples = sample_balanced_dataset(df_all, SAMPLES_PER_EMOTION)\nprint(f\"\\n✅ Total selected samples: {len(selected_samples)}\")\n\n# ================================\n# 3. AUDIO PROCESSING FUNCTIONS\n# ================================\n\ndef apply_conservative_perturbations(audio, sr):\n    \"\"\"Apply conservative perturbations to audio\"\"\"\n    perturbations = []\n    \n    # Random selection of 1-2 perturbations\n    available_perturbations = ['noise', 'pitch', 'time_stretch', 'volume']\n    selected = random.sample(available_perturbations, k=random.randint(1, 2))\n    \n    for perturbation in selected:\n        if perturbation == 'noise':\n            # Add subtle white noise (SNR 20-25 dB)\n            noise_level = random.uniform(0.002, 0.005)\n            noise = np.random.normal(0, noise_level, len(audio))\n            audio = audio + noise\n            perturbations.append(f\"noise_{noise_level:.3f}\")\n            \n        elif perturbation == 'pitch':\n            # Subtle pitch shift (±1 semitone)\n            n_steps = random.uniform(-0.3, 0.3)\n            audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n            perturbations.append(f\"pitch_{n_steps:.2f}\")\n            \n        elif perturbation == 'time_stretch':\n            # Subtle time stretching (0.97-1.03x)\n            rate = random.uniform(0.98, 1.02)\n            audio = librosa.effects.time_stretch(audio, rate=rate)\n            perturbations.append(f\"stretch_{rate:.3f}\")\n            \n        elif perturbation == 'volume':\n            # Subtle volume scaling (±3 dB)\n            db_change = random.uniform(-2, 2)\n            scale_factor = 10 ** (db_change / 20)\n            audio = audio * scale_factor\n            perturbations.append(f\"volume_{db_change:.1f}dB\")\n    \n    return audio, perturbations\n\ndef normalize_audio_duration(audio, sr, target_duration=3.0):\n    \"\"\"Normalize audio to target duration\"\"\"\n    target_length = int(target_duration * sr)\n    \n    if len(audio) > target_length:\n        # Take center portion\n        start = (len(audio) - target_length) // 2\n        audio = audio[start:start + target_length]\n    elif len(audio) < target_length:\n        # Pad with silence\n        pad_length = target_length - len(audio)\n        audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)\n    \n    return audio\n\ndef apply_loudness_normalization(audio, target_lufs=-23.0):\n    \"\"\"Apply loudness normalization (simplified version)\"\"\"\n    # RMS-based normalization as approximation\n    rms = np.sqrt(np.mean(audio**2))\n    if rms > 0:\n        # Target RMS corresponding to approximately -23 LUFS\n        target_rms = 0.1  # Adjust based on testing\n        scale_factor = target_rms / rms\n        audio = audio * scale_factor\n    \n    # Prevent clipping\n    max_val = np.max(np.abs(audio))\n    if max_val > 0.95:\n        audio = audio * (0.95 / max_val)\n    \n    return audio\n\n# ================================\n# 4. PROCESS ALL SAMPLES\n# ================================\n\ndef process_audio_file(sample_info, output_idx):\n    \"\"\"Process a single audio file\"\"\"\n    try:\n        # Load audio\n        audio, orig_sr = librosa.load(sample_info['filepath'], sr=None)\n        \n        # Resample to target sample rate\n        if orig_sr != TARGET_SR:\n            audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=TARGET_SR)\n        \n        # Normalize duration to 3 seconds\n        audio = normalize_audio_duration(audio, TARGET_SR, TARGET_DURATION)\n        \n        # Apply conservative perturbations\n        audio, perturbations = apply_conservative_perturbations(audio, TARGET_SR)\n        \n        # Apply loudness normalization\n        audio = apply_loudness_normalization(audio)\n        \n        # Generate output filename\n        output_filename = f\"val_{output_idx:03d}.wav\"\n        output_path = os.path.join(AUDIO_DIR, output_filename)\n        \n        # Save processed audio\n        sf.write(output_path, audio, TARGET_SR)\n        \n        # Create metadata record\n        metadata = {\n            'filename': output_filename,\n            'emotion_label': sample_info['emotion'],\n            'original_filename': sample_info['filename'],\n            'original_dataset': sample_info['dataset'],\n            'actor_id': sample_info['actor_id'],\n            'original_emotion_code': sample_info['original_emotion_code'],\n            'duration_seconds': TARGET_DURATION,\n            'sample_rate': TARGET_SR,\n            'perturbations_applied': ','.join(perturbations),\n            'processing_success': True\n        }\n        \n        return metadata\n        \n    except Exception as e:\n        print(f\"❌ Error processing {sample_info['filename']}: {str(e)}\")\n        return {\n            'filename': f\"val_{output_idx:03d}.wav\",\n            'emotion_label': sample_info['emotion'],\n            'original_filename': sample_info['filename'],\n            'processing_success': False,\n            'error': str(e)\n        }\n\n# Process all selected samples\nprint(\"\\n🔄 Processing audio files...\")\nprocessed_metadata = []\n\nfor idx, sample in enumerate(selected_samples):\n    if (idx + 1) % 20 == 0:\n        print(f\"   Processed {idx + 1}/{len(selected_samples)} files...\")\n    \n    metadata = process_audio_file(sample, idx + 1)\n    processed_metadata.append(metadata)\n\n# ================================\n# 5. CREATE OUTPUT CSV AND SUMMARY\n# ================================\n\n# Create comprehensive CSV\ndf_output = pd.DataFrame(processed_metadata)\n\n# Add additional useful columns\ndf_output['emotion_id'] = df_output['emotion_label'].map({\n    'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, \n    'neutral': 4, 'sad': 5, 'surprise': 6\n})\n\n# Save CSV\ncsv_path = os.path.join(OUTPUT_DIR, 'validation_labels.csv')\ndf_output.to_csv(csv_path, index=False)\n\n# Create summary statistics\nsuccessful_files = df_output[df_output['processing_success'] == True]\n\nprint(\"\\n📋 VALIDATION DATASET SUMMARY\")\nprint(\"=\" * 50)\nprint(f\"Total files processed: {len(df_output)}\")\nprint(f\"Successfully processed: {len(successful_files)}\")\nprint(f\"Audio specifications: {TARGET_SR}Hz, {TARGET_DURATION}s duration\")\n\nprint(f\"\\n📊 Emotion Distribution:\")\nemotion_counts = successful_files['emotion_label'].value_counts().sort_index()\nfor emotion, count in emotion_counts.items():\n    print(f\"   {emotion.capitalize()}: {count} samples\")\n\nprint(f\"\\n📊 Dataset Source Distribution:\")\nsource_counts = successful_files['original_dataset'].value_counts()\nfor source, count in source_counts.items():\n    print(f\"   {source}: {count} samples\")\n\nprint(f\"\\n📊 Perturbation Statistics:\")\nall_perturbations = []\nfor perturbs in successful_files['perturbations_applied'].dropna():\n    if perturbs:\n        all_perturbations.extend(perturbs.split(','))\n\nfrom collections import Counter\nperturbation_counts = Counter([p.split('_')[0] for p in all_perturbations])\nfor perturbation, count in perturbation_counts.items():\n    print(f\"   {perturbation}: {count} applications\")\n\nprint(f\"\\n💾 Output Files:\")\nprint(f\"   Audio files: {OUTPUT_DIR}/audio_files/ ({len(successful_files)} WAV files)\")\nprint(f\"   Metadata CSV: {csv_path}\")\n\nprint(f\"\\n✅ Validation dataset generation complete!\")\nprint(f\"📁 Files saved to: {OUTPUT_DIR}/\")\n\n# Display first few rows of the CSV for verification\nprint(f\"\\n📋 Sample CSV contents:\")\nprint(successful_files[['filename', 'emotion_label', 'original_dataset', 'actor_id']].head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T20:38:19.268904Z","iopub.execute_input":"2025-05-30T20:38:19.269277Z","iopub.status.idle":"2025-05-30T20:38:28.746813Z","shell.execute_reply.started":"2025-05-30T20:38:19.269251Z","shell.execute_reply":"2025-05-30T20:38:28.745881Z"}},"outputs":[{"name":"stdout","text":"🎵 Speech Emotion Recognition Validation Dataset Generator\n============================================================\n📂 Loading CREMA-D dataset...\n   Found 7442 CREMA-D files\n📂 Loading RAVDESS dataset...\n   Found 2880 RAVDESS files\n\n📊 Dataset Overview:\ndataset   CREMA-D  RAVDESS\nemotion                   \nangry        1271      384\ndisgust      1271      384\nfear         1271      384\nhappy        1271      384\nneutral      1087      576\nsad          1271      384\nsurprise        0      384\n\nTotal files loaded: 10322\nEmotions available in both datasets: {'neutral', 'angry', 'disgust', 'sad', 'happy', 'fear'}\nEmotions only in RAVDESS: {'surprise'}\nEmotions only in CREMA-D: set()\n\n🎲 Creating balanced sample...\n\n🎯 Sampling angry emotion:\n   Available CREMA-D files: 1271\n   Available RAVDESS files: 384\n   Sampling 15 from CREMA-D, 15 from RAVDESS\n   ✅ Selected: 30 files\n   📊 Datasets used: CREMA-D, RAVDESS\n   🎭 Unique actors: 23\n\n🎯 Sampling disgust emotion:\n   Available CREMA-D files: 1271\n   Available RAVDESS files: 384\n   Sampling 15 from CREMA-D, 15 from RAVDESS\n   ✅ Selected: 30 files\n   📊 Datasets used: CREMA-D, RAVDESS\n   🎭 Unique actors: 23\n\n🎯 Sampling fear emotion:\n   Available CREMA-D files: 1271\n   Available RAVDESS files: 384\n   Sampling 15 from CREMA-D, 15 from RAVDESS\n   ✅ Selected: 30 files\n   📊 Datasets used: CREMA-D, RAVDESS\n   🎭 Unique actors: 23\n\n🎯 Sampling happy emotion:\n   Available CREMA-D files: 1271\n   Available RAVDESS files: 384\n   Sampling 15 from CREMA-D, 15 from RAVDESS\n   ✅ Selected: 30 files\n   📊 Datasets used: CREMA-D, RAVDESS\n   🎭 Unique actors: 23\n\n🎯 Sampling neutral emotion:\n   Available CREMA-D files: 1087\n   Available RAVDESS files: 576\n   Sampling 15 from CREMA-D, 15 from RAVDESS\n   ✅ Selected: 30 files\n   📊 Datasets used: CREMA-D, RAVDESS\n   🎭 Unique actors: 24\n\n🎯 Sampling sad emotion:\n   Available CREMA-D files: 1271\n   Available RAVDESS files: 384\n   Sampling 15 from CREMA-D, 15 from RAVDESS\n   ✅ Selected: 30 files\n   📊 Datasets used: CREMA-D, RAVDESS\n   🎭 Unique actors: 25\n\n🎯 Sampling surprise emotion:\n   Available RAVDESS files: 384\n   ✅ Selected: 30 files\n   📊 Datasets used: RAVDESS\n   🎭 Unique actors: 16\n\n✅ Total selected samples: 210\n\n🔄 Processing audio files...\n   Processed 20/210 files...\n   Processed 40/210 files...\n   Processed 60/210 files...\n   Processed 80/210 files...\n   Processed 100/210 files...\n   Processed 120/210 files...\n   Processed 140/210 files...\n   Processed 160/210 files...\n   Processed 180/210 files...\n   Processed 200/210 files...\n\n📋 VALIDATION DATASET SUMMARY\n==================================================\nTotal files processed: 210\nSuccessfully processed: 210\nAudio specifications: 16000Hz, 3.0s duration\n\n📊 Emotion Distribution:\n   Angry: 30 samples\n   Disgust: 30 samples\n   Fear: 30 samples\n   Happy: 30 samples\n   Neutral: 30 samples\n   Sad: 30 samples\n   Surprise: 30 samples\n\n📊 Dataset Source Distribution:\n   RAVDESS: 120 samples\n   CREMA-D: 90 samples\n\n📊 Perturbation Statistics:\n   noise: 74 applications\n   pitch: 77 applications\n   volume: 80 applications\n   stretch: 81 applications\n\n💾 Output Files:\n   Audio files: validation_dataset/audio_files/ (210 WAV files)\n   Metadata CSV: validation_dataset/validation_labels.csv\n\n✅ Validation dataset generation complete!\n📁 Files saved to: validation_dataset/\n\n📋 Sample CSV contents:\n      filename emotion_label original_dataset actor_id\n0  val_001.wav         angry          CREMA-D     1042\n1  val_002.wav         angry          CREMA-D     1034\n2  val_003.wav         angry          CREMA-D     1026\n3  val_004.wav         angry          CREMA-D     1087\n4  val_005.wav         angry          CREMA-D     1070\n5  val_006.wav         angry          CREMA-D     1052\n6  val_007.wav         angry          CREMA-D     1040\n7  val_008.wav         angry          CREMA-D     1025\n8  val_009.wav         angry          CREMA-D     1026\n9  val_010.wav         angry          CREMA-D     1017\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import shutil\nshutil.make_archive('validation_dataset', 'zip', 'validation_dataset')\nprint(\"📦 Validation dataset zipped! Download 'validation_dataset.zip' from the output section.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T20:38:59.246938Z","iopub.execute_input":"2025-05-30T20:38:59.247292Z","iopub.status.idle":"2025-05-30T20:39:00.277888Z","shell.execute_reply.started":"2025-05-30T20:38:59.247268Z","shell.execute_reply":"2025-05-30T20:39:00.276712Z"}},"outputs":[{"name":"stdout","text":"📦 Validation dataset zipped! Download 'validation_dataset.zip' from the output section.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}