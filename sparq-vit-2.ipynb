{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12238365,"sourceType":"datasetVersion","datasetId":7711054},{"sourceId":444327,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":360751,"modelId":381876}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ─── Cell 1: Imports & Globals ───────────────────────────────────────────────\n\n!pip install librosa einops\n\nimport os, sys\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport librosa\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom sklearn.metrics import accuracy_score, classification_report\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Paths (adjust if needed)\nDATA_DIR   = '/kaggle/input/noloudnormser/'\nAUDIO_DIR  = os.path.join(DATA_DIR, 'audio_files')\nLABEL_CSV  = os.path.join(DATA_DIR, 'validation_labels.csv')\nCKPT_PATH  = '/kaggle/input/student_94.07_crema_d.ckpt/pytorch/default/1/student_94.07_CREMA_D.ckpt'  # or '/kaggle/input/.../student_94.07_CREMA_D.ckpt'\nSPEC_DIR   = '/kaggle/working/spec_images'\nos.makedirs(SPEC_DIR, exist_ok=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── Cell 2: Load & Filter Labels ────────────────────────────────────────────\n\ndf = pd.read_csv(LABEL_CSV)\n# keep only the six target emotions\ndf = df[df['emotion_label'] != 'surprise'].reset_index(drop=True)\n\n# Quick sanity\nprint(f\"Total samples: {len(df)}\")\nprint(df['emotion_label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:02:34.526776Z","iopub.execute_input":"2025-06-21T14:02:34.527438Z","iopub.status.idle":"2025-06-21T14:02:34.553238Z","shell.execute_reply.started":"2025-06-21T14:02:34.527410Z","shell.execute_reply":"2025-06-21T14:02:34.552560Z"}},"outputs":[{"name":"stdout","text":"Total samples: 180\nemotion_label\nangry      30\ndisgust    30\nfear       30\nhappy      30\nneutral    30\nsad        30\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ─── New Cell 3: “As‐written” Log‑Mel per paper (magnitude→20·log₁₀, no clip/scale, center‐crop) ────────────\n\nspec_tensors = []\nlabels       = []\n\nfor _, row in df.iterrows():\n    # 1. Load & pad audio to 4 s\n    y, _ = librosa.load(os.path.join(AUDIO_DIR, row['filename']), sr=16000)\n    if len(y) < 4*16000:\n        y = np.pad(y, (0, 4*16000 - len(y)), mode='constant')\n    else:\n        y = y[:4*16000]\n\n    # 2. STFT\n    S = librosa.stft(y,\n                     n_fft=1024,\n                     hop_length=64,\n                     win_length=512,\n                     window='hamming')\n\n    # 3. Mel filter on magnitude (power=1.0)\n    M = librosa.feature.melspectrogram(\n            S=np.abs(S),\n            sr=16000,\n            n_mels=128,\n            power=1.0\n        )\n\n    # 4. Convert to dB via amplitude_to_db (20·log10)\n    logM = librosa.amplitude_to_db(M, ref=1.0)\n\n    # 5. Center‑crop or pad time‐axis to exactly 128 frames\n    T = logM.shape[1]\n    if T >= 128:\n        start = (T - 128) // 2\n        logM = logM[:, start : start + 128]\n    else:\n        pad_left  = (128 - T) // 2\n        pad_right = 128 - T - pad_left\n        logM = np.pad(logM, ((0,0),(pad_left, pad_right)), mode='constant')\n\n    # 6. Store raw dB values (no clipping, no extra scaling)\n    spec_tensors.append(logM.astype(np.float32))\n    labels.append(int(row['emotion_id']))\n\n# Sanity check\nprint(f\"Built {len(spec_tensors)} examples, each shape {spec_tensors[0].shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:02:37.101844Z","iopub.execute_input":"2025-06-21T14:02:37.102513Z","iopub.status.idle":"2025-06-21T14:02:40.282006Z","shell.execute_reply.started":"2025-06-21T14:02:37.102493Z","shell.execute_reply":"2025-06-21T14:02:40.281296Z"}},"outputs":[{"name":"stdout","text":"Loaded 180 spectrograms, each of shape (128, 128)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# ─── Updated Cell 4: Dataset & DataLoader (raw tensors) ───────────────────\n\nclass RawSpecDataset(Dataset):\n    def __init__(self, specs, targets):\n        self.specs   = specs\n        self.targets = targets\n    def __len__(self):\n        return len(self.specs)\n    def __getitem__(self, idx):\n        # specs[idx] is a 128×128 numpy array in [-1,1]\n        img = torch.from_numpy(self.specs[idx]).unsqueeze(0)  # shape (1,128,128)\n        label    = self.targets[idx]\n        filename = df.loc[idx, 'filename']\n        return {'img': img, 'label': label, 'filename': filename}\n\ndataset = RawSpecDataset(spec_tensors, labels)\nloader  = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=2)\n\n# Quick check\nbatch = next(iter(loader))\nprint(batch['img'].shape, batch['img'].min().item(), batch['img'].max().item())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:02:40.282950Z","iopub.execute_input":"2025-06-21T14:02:40.283224Z","iopub.status.idle":"2025-06-21T14:02:40.373159Z","shell.execute_reply.started":"2025-06-21T14:02:40.283200Z","shell.execute_reply":"2025-06-21T14:02:40.372348Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 1, 128, 128]) -0.9011074304580688 1.0\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Took from vit-pytorch github repository\n# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n# Some modification exist\n\nimport math\nimport torch\nfrom torch import nn\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.norm = nn.LayerNorm(dim)\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        x = self.norm(x)\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n                FeedForward(dim, mlp_dim, dropout = dropout)\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n\n        return self.norm(x)\n\n# CreateCoords took from CoordConv github repository\n# https://github.com/walsvid/CoordConv\n# Some modifications exist\n\ndef CreateCoords(max_bs=32, x_dim=64, y_dim=64, with_r=False, skiptile=False):\n    \"\"\"Add coords to a tensor\"\"\"\n    # self.x_dim = x_dim\n    # self.y_dim = y_dim\n    # self.with_r = with_r\n    # self.skiptile = skiptile\n\n    batch_size_tensor = max_bs  # Get batch size\n                                # If you want larger batch, change max_bs\n\n    xx_ones = torch.ones([1, x_dim], dtype=torch.int32)\n    xx_ones = xx_ones.unsqueeze(-1)\n\n    xx_range = torch.arange(y_dim, dtype=torch.int32).unsqueeze(0)\n    xx_range = xx_range.unsqueeze(1)\n\n    xx_channel = torch.matmul(xx_ones, xx_range)\n    xx_channel = xx_channel.unsqueeze(-1)\n\n    yy_ones = torch.ones([1, y_dim], dtype=torch.int32)\n    yy_ones = yy_ones.unsqueeze(1)\n\n    yy_range = torch.arange(x_dim, dtype=torch.int32).unsqueeze(0)\n    yy_range = yy_range.unsqueeze(-1)\n\n    yy_channel = torch.matmul(yy_range, yy_ones)\n    yy_channel = yy_channel.unsqueeze(-1)\n\n    xx_channel = xx_channel.permute(0, 3, 2, 1)\n    yy_channel = yy_channel.permute(0, 3, 2, 1)\n\n    xx_channel = xx_channel.float() / (x_dim - 1)\n    yy_channel = yy_channel.float() / (y_dim - 1)\n\n    xx_channel = xx_channel * 2 - 1\n    yy_channel = yy_channel * 2 - 1\n\n    coords = torch.cat([xx_channel, yy_channel], dim=1)\n    coords = coords.repeat(batch_size_tensor, 1, 1, 1)\n\n    return coords.to('cuda')\n\ndef sinusoidal_pe(d_model, length):\n    \"\"\"\n    :param d_model: dimension of the model\n    :param length: length of positions\n    :return: length*d_model position matrix\n    \"\"\"\n    if d_model % 2 != 0:\n        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n                         \"odd dim (got dim={:d})\".format(d_model))\n    pe = torch.zeros(length, d_model)\n    position = torch.arange(0, length).unsqueeze(1)\n    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n                         -(math.log(10000.0) / d_model)))\n    pe[:, 0::2] = torch.sin(position.float() * div_term)\n    pe[:, 1::2] = torch.cos(position.float() * div_term)\n\n    return pe.to('cuda')\n\nclass CustomDataset(Dataset):\n    def __init__(self, img_list, trg_list):\n        self.img = img_list\n        self.trg = trg_list\n        self.transforms = transforms.ToTensor()\n\n    def __len__(self):\n        return len(self.img)\n    \n    def __getitem__(self, idx):\n        img = plt.imread(self.img[idx])[:,:,:1]\n        img = self.transforms(img)\n        trg = self.trg[idx]\n        return {\"img\": img, \"trg\": trg}\n\nclass Teacher(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., max_bs = 32):\n        super().__init__()\n        image_height, image_width = image_size\n        patch_height, patch_width = patch_size\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        self.conv_stem = nn.Sequential(\n            nn.Conv2d(channels, 16, 3, 1, 1),\n            nn.InstanceNorm2d(16),\n            nn.GELU(),\n            nn.Conv2d(16, 32, 3, 1, 1),\n            nn.InstanceNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 64, 3, 1, 1),\n            nn.InstanceNorm2d(64),\n            nn.GELU(),\n            nn.Conv2d(64, 32, 3, 1, 1),\n            nn.InstanceNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 16, 3, 1, 1),\n            nn.InstanceNorm2d(16),\n            nn.GELU(),\n            nn.Conv2d(16, 1, 3, 1, 1),\n            nn.InstanceNorm2d(1),\n            nn.GELU(),\n        )\n\n        self.coords = CreateCoords(max_bs=max_bs, x_dim=image_width, y_dim=image_height)\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = (channels+2) * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n            nn.LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim),\n            nn.LayerNorm(dim),\n        )\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n\n        self.mlp_head = nn.Linear(dim, num_classes)\n\n    def encoder(self, img):\n        x = self.conv_stem(img)\n        # x = img\n        x = torch.cat((x,self.coords[:x.size(0)]), dim=1)\n        x = self.to_patch_embedding(x)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n        \n        return x\n    \n    def decoder(self, x):\n        y = x\n        x = self.mlp_head(x)\n\n        return x, y\n\n    def forward(self, img):\n        x = self.encoder(img)\n        x, y = self.decoder(x)\n        return x, y\n\nclass Student(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., max_bs = 32):\n        super().__init__()\n        image_height, image_width = image_size\n        patch_height, patch_width = patch_size\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = (channels+0) * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n            nn.LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim),\n            nn.LayerNorm(dim),\n        )\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n\n        self.mlp_head = nn.Linear(dim, num_classes)\n\n    def encoder(self, img):\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n        \n        return x\n    \n    def decoder(self, x):\n        y = x\n        x = self.mlp_head(x)\n\n        return x, y\n\n    def forward(self, img):\n        x = self.encoder(img)\n        x, y = self.decoder(x)\n        return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:02:44.845380Z","iopub.execute_input":"2025-06-21T14:02:44.845681Z","iopub.status.idle":"2025-06-21T14:02:44.875581Z","shell.execute_reply.started":"2025-06-21T14:02:44.845654Z","shell.execute_reply":"2025-06-21T14:02:44.874910Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# ─── Cell 5: Instantiate Model & Load Checkpoint (UPDATED) ────────────\n\n# Student class is already defined above in your pasted model.py\nstudent = Student(\n    image_size=(128,128),\n    patch_size=(128,1),\n    num_classes=6,    # angry,disgust,fear,happy,neutral,sad\n    dim=256,\n    depth=3,\n    heads=5,\n    mlp_dim=256,      # <-- MATCHES checkpoint’s hidden-dim\n    channels=1,\n    emb_dropout=0.,\n    dropout=0.\n).to(device)\n\n# Load the CREMA-D student checkpoint\nckpt = torch.load(CKPT_PATH, map_location=device)\nstudent.load_state_dict(ckpt['model_state_dict'])\nstudent.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:02:47.126494Z","iopub.execute_input":"2025-06-21T14:02:47.126785Z","iopub.status.idle":"2025-06-21T14:02:47.175150Z","shell.execute_reply.started":"2025-06-21T14:02:47.126766Z","shell.execute_reply":"2025-06-21T14:02:47.174541Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"Student(\n  (to_patch_embedding): Sequential(\n    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=128, p2=1)\n    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (2): Linear(in_features=128, out_features=256, bias=True)\n    (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  )\n  (dropout): Dropout(p=0.0, inplace=False)\n  (transformer): Transformer(\n    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    (layers): ModuleList(\n      (0-2): 3 x ModuleList(\n        (0): Attention(\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (attend): Softmax(dim=-1)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (to_qkv): Linear(in_features=256, out_features=960, bias=False)\n          (to_out): Sequential(\n            (0): Linear(in_features=320, out_features=256, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): FeedForward(\n          (net): Sequential(\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (1): Linear(in_features=256, out_features=256, bias=True)\n            (2): GELU(approximate='none')\n            (3): Dropout(p=0.0, inplace=False)\n            (4): Linear(in_features=256, out_features=256, bias=True)\n            (5): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (mlp_head): Linear(in_features=256, out_features=6, bias=True)\n)"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# ─── Cell 6: Inference & Evaluation ─────────────────────────────────────────\n\nall_files, all_true, all_pred = [], [], []\n\nwith torch.no_grad():\n    for batch in loader:\n        imgs = batch['img'].to(device)            # (B,1,128,128)\n        logits, _ = student(imgs)                 # (B,6)\n        preds = logits.argmax(dim=-1).cpu().numpy()\n        all_files.extend(batch['filename'])\n        all_true.extend(batch['label'])\n        all_pred.extend(preds.tolist())\n\n# Save predictions\nresults = pd.DataFrame({\n    'filename': all_files,\n    'true_id': all_true,\n    'pred_id': all_pred\n})\nresults.to_csv('/kaggle/working/predictions.csv', index=False)\n\n# Metrics\nprint(f\"Overall accuracy: {accuracy_score(all_true, all_pred):.4f}\")\nprint(classification_report(\n    all_true, all_pred,\n    target_names=['angry','disgust','fear','happy','neutral','sad']\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:02:51.270518Z","iopub.execute_input":"2025-06-21T14:02:51.271016Z","iopub.status.idle":"2025-06-21T14:02:51.442087Z","shell.execute_reply.started":"2025-06-21T14:02:51.270993Z","shell.execute_reply":"2025-06-21T14:02:51.441126Z"}},"outputs":[{"name":"stdout","text":"Overall accuracy: 0.1778\n              precision    recall  f1-score   support\n\n       angry       0.25      0.17      0.20        30\n     disgust       0.15      0.27      0.19        30\n        fear       0.11      0.07      0.08        30\n       happy       0.25      0.07      0.11        30\n     neutral       0.17      0.03      0.06        30\n         sad       0.19      0.47      0.27        30\n\n    accuracy                           0.18       180\n   macro avg       0.19      0.18      0.15       180\nweighted avg       0.19      0.18      0.15       180\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import confusion_matrix\n\n# 1. Load the CSV\ndf = pd.read_csv('/kaggle/working/predictions.csv')\n\n# 2. Robustly extract the integer from each cell (works for \"tensor(3)\" or \"3\")\ndf['true_id'] = df['true_id'].apply(lambda x: int(''.join(filter(str.isdigit, str(x)))))\ndf['pred_id'] = df['pred_id'].apply(lambda x: int(''.join(filter(str.isdigit, str(x)))))\n\n# 3. Compute confusion matrix\nlabels = ['angry','disgust','fear','happy','neutral','sad']\ncm = confusion_matrix(df['true_id'], df['pred_id'], labels=range(len(labels)))\n\n# 4. Display\ncm_df = pd.DataFrame(cm, index=labels, columns=labels)\nprint(\"Confusion Matrix (rows=true, columns=predicted):\\n\")\nprint(cm_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:02:58.832499Z","iopub.execute_input":"2025-06-21T14:02:58.833150Z","iopub.status.idle":"2025-06-21T14:02:58.846083Z","shell.execute_reply.started":"2025-06-21T14:02:58.833118Z","shell.execute_reply":"2025-06-21T14:02:58.845464Z"}},"outputs":[{"name":"stdout","text":"Confusion Matrix (rows=true, columns=predicted):\n\n         angry  disgust  fear  happy  neutral  sad\nangry        5        7     5      0        0   13\ndisgust      3        8     3      2        2   12\nfear         3       15     2      1        0    9\nhappy        2       10     4      2        2   10\nneutral      4        5     2      2        1   16\nsad          3        8     3      1        1   14\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}